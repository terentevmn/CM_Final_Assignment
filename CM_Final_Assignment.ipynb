{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Final Assignment: A Single-Layer and a Multi-Layer Perceptron\n",
        "\n",
        "**Course**: Cognitive Modelling (2022-2023)\n",
        "\n",
        "**Student**: Maksim Terentev (2565137)\n",
        "\n",
        "**Description**: This notebook is the final assignment of the Cognitive Modelling course, where we will build a single-layer perceptron and look at how it performs on binary classification tasks for two simple datasets (linary separable and non-lineary separable). Moreover, we will look at a more advanced feed-forward neural network called a multi-layer perceptron. The MLP will be used to learn non-linearly separable patterns. Finally, we will look at how the MLP performs on the brain activity data (EEGBCI dataset).\n",
        "\n",
        "**Note**: The code of this project is based on [Multi-Layer Perceptron](https://clclab.github.io/FNCM/Lab4-MLP.html)."
      ],
      "metadata": {
        "id": "BgaYd-tFEZFb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting Things up"
      ],
      "metadata": {
        "id": "HDkU8y2eFMjc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For this project, we will need the `NumPy`, `pandas`, `Matplotlib`, `MNE`, `SciPy`, `PyWavelets`, and `scikit-learn` libraries, as well as some functions that are defined in the `MLP.py` file and some methods located in the `EEG.py` file. The cell below takes care of all of this. When the following message appears: \n",
        "```\n",
        "Do you want to set the path: /root/mne_data as the default EEGBCI dataset path in the mne-python config [y]/n?\n",
        "```, press `y` and then `enter`."
      ],
      "metadata": {
        "id": "b-rF4Jg2jmkA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xdDIP5-_ETN4"
      },
      "outputs": [],
      "source": [
        "# Install missing package\n",
        "!pip install mne\n",
        "\n",
        "# Import packages\n",
        "import numpy as np                     # math\n",
        "import pandas as pd                    # data manipulation\n",
        "import matplotlib.pyplot as plt        # plotting\n",
        "import mne                             # EEG dataset\n",
        "import scipy                           # scientific computing & signal processing\n",
        "import pywt                            # wavelet transform\n",
        "import random                          # shuffling lists\n",
        "import sklearn                         # machine learning tools (feature selection)\n",
        "\n",
        "# Download code from the FNCM github repository\n",
        "!wget --no-cache {\"https://raw.githubusercontent.com/clclab/FNCM/main/book/Lab4-materials/EEG.py\"}\n",
        "!wget --no-cache {\"https://raw.githubusercontent.com/clclab/FNCM/main/book/Lab4-materials/MLP.py\"}\n",
        "\n",
        "# Import downloaded packages\n",
        "import MLP\n",
        "from MLP import MLP, plot_errors\n",
        "import EEG\n",
        "from EEG import full_epochs, cropped_epochs, EEG_imagery"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introducing Data"
      ],
      "metadata": {
        "id": "8IYmYQOQwO43"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will train our perceptron on logical OR and XOR datasets. The difference between those datasets is that the OR dataset is lineary separable while the XOR dataset is non-lineary separable. Both datasets are defined in the two cells below."
      ],
      "metadata": {
        "id": "Wh4krmHZnT9S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# OR data\n",
        "data_OR = pd.DataFrame(data = {'x1': [1, 1, 0, 0], \n",
        "                             'x2': [1, 0, 1, 0],\n",
        "                             'y': [1, 1, 1, 0]})\n",
        "data_OR"
      ],
      "metadata": {
        "id": "abyduhPDwQ0D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# XOR data\n",
        "data_XOR = pd.DataFrame(data = {'x1': [1, 1, 0, 0], \n",
        "                              'x2': [1, 0, 1, 0],\n",
        "                              'y': [0, 1, 1, 0]})\n",
        "\n",
        "data_XOR"
      ],
      "metadata": {
        "id": "4eNg9F98wpHS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will construct a single-layer perceptron in the cells below and determine whether the perceptron can learn the pattern on each of datasets."
      ],
      "metadata": {
        "id": "FU5A6DvEfBOJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introducing a Single-Layer Perceptron"
      ],
      "metadata": {
        "id": "q5-OQOk6w0KG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, we will define the activation functions. The `binary_threshold(x)` activation function will be used for the single-layer perceptron and the `sigmoid(x)` activation function will be used for the multi-layer perceptron."
      ],
      "metadata": {
        "id": "uwshyChonjrD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# binary threshold function\n",
        "def binary_threshold(x):\n",
        "    \"\"\"\n",
        "    Binary threshold function.\n",
        "    \n",
        "    Input:\n",
        "      x -- an array\n",
        "      \n",
        "    Output:\n",
        "      binary_threshold(x) = 1.0 if x > 0, else 0.0\n",
        "    \"\"\"\n",
        "    return 1. * (x > 0)\n",
        "\n",
        "# sigmoid function\n",
        "def sigmoid(x):\n",
        "    \"\"\"\n",
        "    Sigmoid function.\n",
        "    \n",
        "    Input:\n",
        "      x -- an array\n",
        "      \n",
        "    Output:\n",
        "      sigmoid(x) = 1/(1 + exp(-x))\n",
        "    \"\"\"\n",
        "    return 1./(1. + np.exp(-x))"
      ],
      "metadata": {
        "id": "lgMABhIRwyGh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us now introduce a single-layer perceptron represented through the `Perceptron` class."
      ],
      "metadata": {
        "id": "fNiuLIq4n1-a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perceptron model\n",
        "class Perceptron:\n",
        "    def __init__(self, dim, activation):\n",
        "        \"\"\"\n",
        "        Initializes the perceptron -- sets weights, bias and\n",
        "        activation function; returns the perceptron object.\n",
        "        \n",
        "        Input:\n",
        "          dim -- dimensionality of the datapoints (integer)\n",
        "          activation -- the desired activation function (function)\n",
        "        \"\"\"\n",
        "        \n",
        "        # initialize all weights and bias as zero\n",
        "        self.weights = np.zeros(dim)\n",
        "        self.bias = 0\n",
        "        \n",
        "        # activation function\n",
        "        self.activation = activation\n",
        "            \n",
        "    def compute_error(self, X, y):\n",
        "        \"\"\"\n",
        "        Computes the mean squared error (MSE) of the perceptron with regard \n",
        "        to the dataset.\n",
        "        \n",
        "        Input:\n",
        "          X -- an (N, D) array in which each row is a data point\n",
        "          y -- an (N, 1) array in which each row is the target of X[i,]\n",
        "          \n",
        "        Output:\n",
        "          classification error (mean squared error)\n",
        "        \"\"\"\n",
        "        \n",
        "        # number of data points\n",
        "        N = len(X)\n",
        "        \n",
        "        # compute mean squared error\n",
        "        mse = np.sum((y.flatten() - \\\n",
        "                self.activation(X @ self.weights + self.bias)) ** 2) / N\n",
        "        \n",
        "        return mse\n",
        "    \n",
        "    def plot(self, X, y, example_input):\n",
        "        \"\"\"\n",
        "        Plot the data points and the (learned) decision boundary.\n",
        "        \"\"\"\n",
        "        targets = y.flatten()\n",
        "        wmin = -1.5\n",
        "        wmax = 1.5\n",
        "        \n",
        "        # plot data points\n",
        "        plt.plot(X[targets == 1, 0], X[targets == 1, 1], 'bo', ms = 10)\n",
        "        plt.plot(X[targets == 0, 0], X[targets == 0, 1], 'rs', ms = 10)\n",
        "        plt.xlim(wmin, wmax)\n",
        "        plt.ylim(wmin, wmax)\n",
        "        plt.xticks([0, 1])\n",
        "        plt.yticks([0, 1])\n",
        "        plt.xlabel('$x_1$', size=15)\n",
        "        plt.ylabel('$x_2$', size=15)\n",
        "        \n",
        "        # plot circle around current example\n",
        "        plt.scatter(example_input[0], example_input[1], marker = 's',\n",
        "                    color = 'none', edgecolor = 'g', s = 400)\n",
        "        \n",
        "        # plot decision boundary\n",
        "        if perceptron.weights[0] == 0. and perceptron.weights[1] == 0:\n",
        "            pass\n",
        "        elif perceptron.weights[1] == 0.:\n",
        "            db_y = np.arange(start = wmin, stop = wmax, step = 0.1)\n",
        "            db_x = (-perceptron.bias - db_y * perceptron.weights[1]) / \\\n",
        "                    perceptron.weights[0]\n",
        "            plt.plot(db_x, db_y, 'k-')\n",
        "        else:\n",
        "            db_x = np.arange(start = wmin, stop = wmax, step = 0.1)\n",
        "            db_y = (-perceptron.bias - db_x * perceptron.weights[0]) / \\\n",
        "                    perceptron.weights[1]\n",
        "            plt.plot(db_x, db_y, 'k-')\n",
        "        plt.show()\n",
        "    \n",
        "    def train(self, X, y, epochs, learn_rate = 0.1, stepbystep = True):\n",
        "        \"\"\"\n",
        "        Trains the perceptron -- iterates over a dataset and learns \n",
        "        online.\n",
        "        \n",
        "        Input:\n",
        "          X -- an (N,D) array in which each row is a data point\n",
        "          y -- an (N,1) array in which each row is the target of X[i,]\n",
        "          epochs -- the number of epochs (iterations) for training\n",
        "          learn_rate -- the learning rate for training\n",
        "          stepbystep -- wait for user to continue and print each step\n",
        "          \n",
        "        Output:\n",
        "          errors -- error over the entire data after each iteration\n",
        "        \"\"\"\n",
        "        errors = np.zeros(epochs)\n",
        "        \n",
        "        for it in range(0, epochs):\n",
        "            if stepbystep:\n",
        "                print('iteration', it + 1)\n",
        "            \n",
        "            # pick training example\n",
        "            i = it % len(X)\n",
        "            example_input = X[i, : ]\n",
        "            true_label = y[i]\n",
        "            \n",
        "            # update perceptron\n",
        "            self.update(example_input, true_label, learn_rate, stepbystep)\n",
        "            \n",
        "            if stepbystep:\n",
        "                self.plot(X, y, example_input)\n",
        "                inpt = input()\n",
        "                if inpt == 'q':\n",
        "                    print('Stopped after', it + 1, 'iterations.')\n",
        "                    break\n",
        "            \n",
        "            errors[it] = self.compute_error(X, y)\n",
        "            \n",
        "        return errors\n",
        "    \n",
        "    def update(self, example_input, true_label, learn_rate, stepbystep = True):\n",
        "        \"\"\"\n",
        "        Applies a single update to the perceptron.\n",
        "        \n",
        "        Input:\n",
        "          example_input -- an example datapoint (vector)\n",
        "          true_label -- the corresponding label (ground truth)\n",
        "          learn_rate -- the learning rate (real number)\n",
        "          stepbystep -- whether to print the results\n",
        "        \"\"\"\n",
        "        pred = self.activation(np.sum(self.weights.T * example_input + self.bias))\n",
        "\n",
        "        self.weights = self.weights + learn_rate * (true_label - pred) * example_input\n",
        "        self.bias = self.bias + learn_rate * (true_label - pred)\n",
        "        \n",
        "        if stepbystep:\n",
        "            print('\\tinput:', example_input)\n",
        "            print('\\ttarget:', true_label)\n",
        "            print('\\tprediction:', pred)\n",
        "            print('\\tweights:', self.weights)\n",
        "            print('\\tbias:', self.bias, '\\n')"
      ],
      "metadata": {
        "id": "T3Q5iPR2n8sE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training a Single-Layer Perceptron"
      ],
      "metadata": {
        "id": "rpVuEKEEpMJz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us train the perceptron on both datasets and see how it performs on each of them. "
      ],
      "metadata": {
        "id": "aTNPxqb-uwUH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# the data\n",
        "# change between data_OR and data_XOR based on what you want to learn\n",
        "data = data_XOR\n",
        "\n",
        "# shuffle data\n",
        "np.random.shuffle(data.values)\n",
        "X = data[['x1', 'x2']].values\n",
        "y = data[['y']].values\n",
        "input_size = X.shape[1]\n",
        "\n",
        "# initialize the perceptron\n",
        "perceptron = Perceptron(dim = input_size,\n",
        "                        activation = binary_threshold)\n",
        "\n",
        "# print model attributes before training\n",
        "print('weights:', perceptron.weights)\n",
        "print('bias:', perceptron.bias)\n",
        "print('activation function:', perceptron.activation.__name__)\n",
        "\n",
        "# train perceptron\n",
        "perceptron.train(X, y,\n",
        "                 epochs = 15,\n",
        "                 learn_rate = 0.1,\n",
        "                 stepbystep = True)"
      ],
      "metadata": {
        "id": "wH5Ujz0vxLyS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introducing and Training a Multi-Layer Perceptron"
      ],
      "metadata": {
        "id": "gGVC95q24Z8Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For the perceptron to solve the XOR problem, it needs to have multiple layers, and thus a multi-layer perceptron (MLP) will be introduced to handle non-linearly separable data. An MLP is a neural network in which neuron layers are stacked such that the output of a neuron in a layer is only allowed to be an input to neurons in the upper layer. The MLP is represented as the `MLP()` class and can be found in the `MLP.py` file.\n",
        "\n",
        "Let us now train the MLP on both datasets."
      ],
      "metadata": {
        "id": "3kHiyUg2qbK1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# the data\n",
        "# change between data_OR and data_XOR based on what you want to learn\n",
        "data = data_OR\n",
        "\n",
        "# shuffle data\n",
        "np.random.shuffle(data.values)\n",
        "X = data[['x1', 'x2']].values\n",
        "y = data[['y']].values\n",
        "input_size = X.shape[1]\n",
        "\n",
        "# initialize the MLP\n",
        "mlp = MLP(n_input = input_size, \n",
        "          n_hidden = 5, # number of hidden layers\n",
        "          n_output = 1) # number of outputnodes\n",
        "\n",
        "# train the MLP on the dataset\n",
        "errors = mlp.train(X, y, \n",
        "                   learn_rate = 0.2,\n",
        "                   maxit = 700) # number of training epochs\n",
        "\n",
        "# make predictions on the training dataset\n",
        "predictions = np.array([mlp.predict(X[i, ]) for i in range(len(X))])\n",
        "\n",
        "# compute final accuracy\n",
        "accuracy = sum((predictions > 0.5).astype(int) == y) / len(y)\n",
        "print('Final accuracy:', accuracy)\n",
        "\n",
        "# plot weighed SSE over iterations\n",
        "plot_errors(errors, 'Weighed SSE')"
      ],
      "metadata": {
        "id": "Z5lCi80V4e7I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training a MLP on EEG Data"
      ],
      "metadata": {
        "id": "Osqq1Lsy4x7i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will now look at how well the MLP can be trained to classify data we did not create ourselves but recorded from brain activity. We will use a dataset from the [MNE](https://mne.tools/stable/index.html) library. The dataset (EEGBCI, described at [PhysioNet](https://www.physionet.org/content/eegmmidb/1.0.0/)) contains EEG recordings on various tasks, including a *motor imagery* task: subjects were instructed to imagine moving either their hands or their feet, given some visual cue. We will use the data from one participant performing this task in 45 trials (21 for hands, 24 for feet), while EEGs were recorded from 64 electrodes across the scalp. \n",
        "\n",
        "First, let us load the data."
      ],
      "metadata": {
        "id": "VWbY8G7hPUah"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "feet_plot = full_epochs['feet'].average().plot()"
      ],
      "metadata": {
        "id": "FKKzqH8y40Sk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hands_plot = full_epochs['hands'].average().plot()"
      ],
      "metadata": {
        "id": "Rf3BIRWu43lP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have loaded the EEG data from 0.5 seconds before the visual cue was presented until 0.5 seconds after it was presented. The plots above show the raw EEG data for each of the 64 channels, averaged over trials, with the cue presentation at $t=0$. On the x-axis, time in seconds (s) can be seen; on the y-axis, the recorded signal in millivolts (mV)."
      ],
      "metadata": {
        "id": "-fj2VStv10c3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We would like the MLP to learn to distinguish this pattern or, in other words, to predict from an EEG signal which extremities a subject is imaginarily moving after a signal has been introduced. For this purpose, we will only use the signals where the subject was actually imagining the movement ($t=0$). Furthermore, we don’t use all data points in the raw EEG signal; instead, we compute some features with which we aim to capture the most critical information about the wave.\n",
        "\n",
        "We intend to capture some of the temporal properties of the EEG signal (how the wave changes over time), as well as some spectral properties (how intense the signal is in different frequency bands). \n",
        "\n",
        "For the *temporal properties*, we divide the y-axis range of the EEG signal into three quantiles (in the plot below, the regions are divided by the grey horizontal lines). From every raw EEG wave (the black line), we first remove the noise (giving the blue line) and then compute the average of the transformed signal at a few evenly spread points (blue dots). For each of those dots, we add a 0, 1, or 2 to our list of features depending on what quantile they fall in. Hence, the list of temporal features encoding the signal below would be `[0, 1, 1, 0, 0, 0, 1, 2, 2, 2, 2]`. The quantiles are computed separately for each channel of every epoch, such that there will be an approximately equal number of points in each of the three ranges — the 0s, 1s, and 2s, therefore encode the _relative_ upward and downward movement of the wave over time.\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/clclab/FNCM/main/book/Lab4-materials/img/tempf.png\" alt=\"Temporal feature extraction\" width=\"800\"/>\n",
        "\n",
        "For the *spectral properties*, we look at three different frequency bands: theta (3.5 - 7.5 Hz), alpha (7.5 - 13 Hz), and beta (14+ Hz). We want to encode the relative distribution of spectral power density in these three bands: which frequency band contains the most movement and which the least? Again, we encode this using 0, 1, and 2, but this time, the numbers mean “least,” “middle,” and “most” spectral power density. The plot below shows the spectral power density for different frequency ranges across the entire signal timeframe. The alpha band has the highest power density for this signal, followed by theta and beta. The list of spectral features encoding this signal would therefore be `[1, 2, 0]`.\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/clclab/FNCM/main/book/Lab4-materials/img/specf.png\" alt=\"Spectral feature extraction\" width=\"800\"/>\n",
        "\n",
        "Ultimately, we select the best of all spectral and temporal features across trials, such that we end up with a list of 96 numbers (all 0s, 1s, and 2s) encoding the EEG signal per trial. More information about the feature extraction and selection procedure can be found in the `EEG.py`.\n",
        "\n",
        "This is all information we need for the MLP to learn to distinguish between imagined hand or foot movements.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "VltqZ4cW2trY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We split the data into a training and a test set to see how well the MLP model can learn to do the classification. We will train the model on two-thirds of the data and then test its prediction accuracy on the other third. As such, we evaluate how well the learned decision boundary (in this case, a many-dimensional hypersurface) will generalize to correctly classify unseen data."
      ],
      "metadata": {
        "id": "9tPFOGaM7AZO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# shuffle data\n",
        "idx = list(range(len(EEG_imagery['trial_numbers'])))\n",
        "random.shuffle(idx)\n",
        "EEG_imagery['trial_numbers'] = EEG_imagery['trial_numbers'][idx]\n",
        "EEG_imagery['features'] = EEG_imagery['features'][idx]\n",
        "EEG_imagery['labels'] = EEG_imagery['labels'][idx]\n",
        "\n",
        "# split 2/3 to train & test set\n",
        "trials_train = EEG_imagery['trial_numbers'][ : 30]\n",
        "trials_test = EEG_imagery['trial_numbers'][30 : ]\n",
        "X_train = EEG_imagery['features'][ : 30]\n",
        "X_test = EEG_imagery['features'][30 : ]\n",
        "y_train = EEG_imagery['labels'][ : 30]\n",
        "y_test = EEG_imagery['labels'][30 : ]"
      ],
      "metadata": {
        "id": "wAqRmakR5ksv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code block below trains the MLP, plots the error over training, and prints the final accuracy on the test set."
      ],
      "metadata": {
        "id": "wMfi-9487iPG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# train MLP & plot training error\n",
        "mlp = MLP(n_input = 96, \n",
        "          n_hidden = 5, \n",
        "          n_output = 1)\n",
        "\n",
        "errors = mlp.train(X_train, y_train, \n",
        "                   maxit = 100, learn_rate = 0.1)\n",
        "\n",
        "plot_errors(errors, 'Weighed SSE')\n",
        "\n",
        "# make predictions on test set\n",
        "predictions = np.array([mlp.predict(X_test[i]) for i in range(len(X_test))]).flatten()\n",
        "accurate = ((predictions > 0.5).astype(int) == y_test)\n",
        "accuracy = sum(accurate) / len(y_test)\n",
        "print('Final accuracy:', accuracy)"
      ],
      "metadata": {
        "id": "oheccoGN5na9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}